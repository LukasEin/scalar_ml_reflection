{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, argparse, os, sys, optuna, itertools, pickle, warnings, logging, time\n",
    "import numpy as np, pytorch_lightning as pl\n",
    "from tqdm.notebook import tqdm\n",
    "from operator import itemgetter\n",
    "from Optuna import *\n",
    "\n",
    "# suppress warning and logging\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.disable(sys.maxsize)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU available: ' + torch.cuda.get_device_name())\n",
    "else:\n",
    "    raise RuntimeError('No GPU found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, the architecture search space is specified. <br>\n",
    "`NUM_TRIALS_PER_TRIAL`: how often the same combination of parameters should be tried (optuna then optimizes over the average loss of these trained models) <br>\n",
    "`MAX_EPOCHS`: maximum number of training epochs for the models <br>\n",
    "`unique_trials`: number of trials; it is made sure that optuna is not allowed to choose the same parameter combinations, i.e. the same architecture, more than once. <br>\n",
    "The `name_string_helper` helps to keep track of the generated .pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = \"../datasets\"\n",
    "\n",
    "# convolutional part\n",
    "min_conv_layers = 1\n",
    "max_conv_layers = 3\n",
    "channels = [10, 13, 16, 19, 22]\n",
    "\n",
    "# dense layer\n",
    "units = []\n",
    "\n",
    "# how often the same combination of parameters should be tried\n",
    "NUM_TRIALS_PER_TRIAL = 3\n",
    "MAX_EPOCHS = 200\n",
    "unique_trials = 50\n",
    "\n",
    "name_string_helper = 'test_run'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions\n",
    "nt, nx = 60, 4\n",
    "\n",
    "# dataset paths\n",
    "train_path = os.path.join(path_data, \"dataset-train-{:d}-{:d}.pt\".format(nt, nx))\n",
    "val_path = os.path.join(path_data, \"dataset-val-{:d}-{:d}.pt\".format(nt, nx))\n",
    "\n",
    "train_data, val_data = torch.load(train_path), torch.load(val_path)\n",
    "\n",
    "print(\"Total training examples: {}\".format(len(train_data)))\n",
    "print(\"Total validation examples: {}\".format(len(val_data)))\n",
    "\n",
    "# define how many training instances should be used\n",
    "train_sample_numbers = [50, 200, 2000, 20000]\n",
    "#train_sample_numbers = list(itertools.chain(range(100, 250, 50), range(250, 1000, 250), range(1000, 3000, 500), range(3000, 20001, 1000)))\n",
    "val_sample_numbers = [int(train_sample_numbers[i]/10) for i in range(len(train_sample_numbers))]\n",
    "train_subsets = [range(train_sample_numbers[i]) for i in range(len(train_sample_numbers))]\n",
    "val_subsets = [range(val_sample_numbers[i]) for i in range(len(val_sample_numbers))]\n",
    "\n",
    "# hyperparameters\n",
    "hparams_defaults = argparse.Namespace()\n",
    "hparams_defaults.num_workers = 0\n",
    "hparams_defaults.lr = 1e-2\n",
    "hparams_defaults.weight_decay = 0.\n",
    "hparams_defaults.name = name_string_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tqdm(total=len(train_sample_numbers)) as pbar_samples:\n",
    "\n",
    "    for train_subset, val_subset in zip(train_subsets, val_subsets):\n",
    "        \n",
    "        # We want to create a new study for each amount of training samples that we defined above.\n",
    "        train_data_subset = torch.utils.data.Subset(train_data, train_subset)\n",
    "        val_data_subset = torch.utils.data.Subset(val_data, val_subset)\n",
    "        print(\"Training examples used: {}\".format(len(train_data_subset)))\n",
    "        print(\"Validation examples used: {}\".format(len(val_data_subset)))\n",
    "\n",
    "        MODEL_DIR = 'optuna/eq_{}_train_samples/'.format(len(train_data_subset))\n",
    "\n",
    "        # The total number of validation samples has to be divisible by the batch size for the loss function\n",
    "        # and the MSE losses to be correctly averaged at validation_epoch_end.\n",
    "        \n",
    "        # The batch size is usually chosen to be 100, but the smallest training set has only 100 training samples.\n",
    "        # In order not to employ batch training (but mini-batch training!) for the smallest set and to improve\n",
    "        # stochasticity in the smaller training sets, the batch size is set to 50 for them.\n",
    "        if len(train_data_subset) < 500:\n",
    "            hparams_defaults.batch_size = 50\n",
    "        else:\n",
    "            hparams_defaults.batch_size = 100\n",
    "\n",
    "        # Create the study.\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        \n",
    "        with tqdm(total=unique_trials) as pbar_unique_trials:\n",
    "            \n",
    "            # Try unique_trials UNIQUE combinations of parameters, not total combinations.\n",
    "            while unique_trials > len(set(str(t.params) for t in study.trials)):\n",
    "                study.optimize(lambda trial: objective(trial, hparams_defaults, train_data_subset, val_data_subset, min_conv_layers, max_conv_layers, channels, units, model_dir=MODEL_DIR, num_trials_per_trial=NUM_TRIALS_PER_TRIAL, max_epochs=MAX_EPOCHS), n_trials=1)\n",
    "                if study.trials[-1].value is not None:\n",
    "                    pbar_unique_trials.update(1)\n",
    "\n",
    "\n",
    "        # Give an overview over the study and the best trial.\n",
    "        print('Number of training data:', len(train_data_subset))\n",
    "        print('Number of finished trials:', len(study.trials))\n",
    "        print('Best trial:')\n",
    "        trial = study.best_trial\n",
    "        print('vloss:', trial.value)\n",
    "        print('Params:')\n",
    "        for key, value in trial.params.items():\n",
    "            print('{}: {}'.format(key, value))\n",
    "        print('\\n')\n",
    "        \n",
    "        # Since we implemented the usage of unique trials by pruning trials for which the combination of parameters\n",
    "        # had already been used before, the pruned trials show up with None as a value for the mean training loss.\n",
    "        # We delete the corresponding trials before sorting by the mean loss and saving the results.\n",
    "        model_results = [[study.trials[i].value, study.trials[i].params] for i in range(len(study.trials))]\n",
    "\n",
    "        for i in range(len(model_results)-1, -1, -1):\n",
    "            if model_results[i][0] is None:\n",
    "                del model_results[i]\n",
    "\n",
    "        model_results.sort(key=itemgetter(0))\n",
    "\n",
    "        pickle_path = 'optuna_pickles/'\n",
    "        if not os.path.isdir(pickle_path):\n",
    "            os.mkdir(pickle_path)\n",
    "        \n",
    "        filename = name_string_helper + '_{}_training_samples.pickle'.format(len(train_data_subset))\n",
    "        if os.path.isfile(os.path.join(pickle_path, filename)):\n",
    "            filename = str(time.time()) + filename\n",
    "            print('File already existed, timestamp was prepended to filename.')\n",
    "        with open(os.path.join(pickle_path, filename), 'wb') as file:\n",
    "            pickle.dump(model_results, file)\n",
    "        pbar_samples.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 ('regression')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "e414d9badab23790d6e50d62656fbecc71bbdc03f340f772773efa90285a9aaf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
