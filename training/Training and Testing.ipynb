{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, argparse, os, sys, time, itertools, pickle, warnings, logging\n",
    "from tqdm.notebook import tqdm\n",
    "from training_and_testing import *\n",
    "import matplotlib.pyplot as plt, numpy as np, pytorch_lightning as pl\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU available: ' + torch.cuda.get_device_name())\n",
    "else:\n",
    "    raise RuntimeError('No GPU found.')\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.disable(sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please choose the path to the datasets in `path_data`. <br>\n",
    "The `name_string_helper` helps to keep track of the different architectures that are trained. <br>\n",
    "`NUM_TRIALS` specifies how many models will be trained of the same architecture. <br>\n",
    "`MIN_EPOCHS` and `MAX_EPOCHS` set the boundaries for the training procedure. <br>\n",
    "\n",
    "## Defining the architecture\n",
    "\n",
    "The specification of the architecture works as follows: <br>\n",
    "`kernels` defines the kernel size of the convolutions in the order they are applied to the input. <br>\n",
    "`channels` specifies the number of output channels of the corresponding convolutions. <br>\n",
    "Note that the number of input channels does not have to be specified because the lattice configuration's number of channels is hard coded in the `ObsPredictor` class, since it is always four. The other numbers of input channels are already fixed by the preceeding output channels. <br>\n",
    "`dense_sizes` specifies the additional dense layers between the output of the convolutional part and the output of the whole network. The input of the dense part of the network is already specified by the last entry of `channels` and whether or not the output of the convolutional layers should be flattened. The output of the whole network is hard coded in the `ObsPredictor` class, since we always want to predict two values, $n$ and $|\\phi|^2$. If one wants the output of the convolutional part of the network with e.g. $48$ channels to be directly connected to the output of the whole network, one would set `channels`$= [\\ldots, 48]$ and `dense_sizes`$=[]$. <br>\n",
    "After every convolution, a Tanh activation function is applied. After every dense layer except directly before the output a LeakyReLu activation function is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path of the datasets\n",
    "path_data = './datasets'\n",
    "\n",
    "# The name_string_helper is part of the model name and also helps to determine the name of the pickle file that will be\n",
    "# generated.\n",
    "name_string_helper = 'test_ref'\n",
    "\n",
    "# NUM_TRIALS determines how often one architecture should be trained, starting from different initializations.\n",
    "NUM_TRIALS = 3\n",
    "\n",
    "MIN_EPOCHS = 100\n",
    "MAX_EPOCHS = 1000\n",
    "\n",
    "\"\"\"\n",
    "    Architecture\n",
    "\"\"\"\n",
    "\n",
    "kernels = [1, 1]\n",
    "channels = [13, 13]\n",
    "dense_sizes = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, no changes should be made. It runs a few checks to test if the vriables above have been chosen correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions of the lattice to train on\n",
    "NT, NX = 60, 4\n",
    "\n",
    "# TEST_BATCH_SIZE determines the batch size that is used during testing. A larger value speeds up the testing process.\n",
    "# The maximum value that can be chosen depends on the graphics card that is used. It is smaller for a larger lattice.\n",
    "# The total number of test samples have to be divisble by it for the results being averaged correctly.\n",
    "TEST_BATCH_SIZE = 500\n",
    "\n",
    "train_path = os.path.join(path_data, \"dataset-train-{:d}-{:d}.pt\".format(NT, NX))\n",
    "val_path = os.path.join(path_data, \"dataset-val-{:d}-{:d}.pt\".format(NT, NX))\n",
    "test_path = os.path.join(path_data, \"dataset-test-{:d}-{:d}.pt\".format(NT, NX))\n",
    "\n",
    "# checks\n",
    "if not len(kernels) == len(channels):\n",
    "    raise ValueError('kernels and channels must have the same length.')\n",
    "\n",
    "if not isinstance(NUM_TRIALS, int):\n",
    "    raise TypeError('NUM_TRIALS has to be an integer.')\n",
    "    \n",
    "if not NUM_TRIALS > 0:\n",
    "    raise ValueError('NUM_TRIALS has to be positive.')\n",
    "    \n",
    "if not isinstance(TEST_BATCH_SIZE, int):\n",
    "    raise TypeError('TEST_BATCH_SIZE has to be an integer.')\n",
    "    \n",
    "if not TEST_BATCH_SIZE > 0:\n",
    "    raise ValueError('TEST_BATCH_SIZE has to be positive.')\n",
    "\n",
    "if not os.path.isfile(train_path):\n",
    "    raise FileNotFoundError('There is no training set of this lattice size under the specified path.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing on the 60x4 lattice\n",
    "\n",
    "Here, the training takes place. Its duration depends particularly on the chosen `NUM_TRIALS` and `train_sample_numbers`. Then, the models are tested on the $60 \\times 4$ lattice. The models and the test results are saved to a .pickle file. <br>\n",
    "In the following cell, only `train_sample_numbers` should be modified. It specifies the number of training samples in the respective training sets and has to respect $0 <$ `train_sample_numbers` $\\le 20000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training will be performed on the {}x{} lattice.'.format(NT, NX))\n",
    "\n",
    "# loading the data\n",
    "train_data, val_data, test_data = torch.load(train_path), torch.load(val_path), torch.load(test_path)\n",
    "\n",
    "print(\"Total training examples: {}\".format(len(train_data)))\n",
    "print(\"Total validation examples: {}\".format(len(val_data)))\n",
    "print(\"Total test examples: {}\".format(len(test_data)))\n",
    "\n",
    "if len(test_data) % TEST_BATCH_SIZE != 0:\n",
    "    raise ValueError(f'The number of test data ({len(test_data)}) has to be a multiple of TEST_BATCH_SIZE ({TEST_BATCH_SIZE}). Please choose the latter accordingly.')\n",
    "    \n",
    "# The different sizes of the training sets are chosen.\n",
    "# The corresponding validation sets have 10% of the training set's size.\n",
    "train_sample_numbers = [50, 200, 2000, 20000]\n",
    "# train_sample_numbers = list(itertools.chain(range(100, 250, 50), range(250, 1000, 250), range(1000, 3000, 500), range(3000, 20001, 1000)))\n",
    "val_sample_numbers = [int(train_sample_numbers[i]/10) for i in range(len(train_sample_numbers))]\n",
    "train_subsets = [range(train_sample_numbers[i]) for i in range(len(train_sample_numbers))]\n",
    "val_subsets = [range(val_sample_numbers[i]) for i in range(len(val_sample_numbers))]\n",
    "\n",
    "\n",
    "test_MSEs = []\n",
    "test_losses = []\n",
    "results = []\n",
    "\n",
    "with tqdm(total=len(train_sample_numbers)) as pbar:\n",
    "    for i, (train_subset, val_subset) in enumerate(zip(train_subsets, val_subsets)):\n",
    "        \n",
    "        \"\"\"\n",
    "            Datasets\n",
    "        \"\"\"\n",
    "        train_data_subset = torch.utils.data.Subset(train_data, train_subset) \n",
    "        val_data_subset = torch.utils.data.Subset(val_data, val_subset)\n",
    "        \n",
    "        print(\"Training examples used: {}\".format(len(train_data_subset)))\n",
    "        print(\"Validation examples used: {}\".format(len(val_data_subset)))\n",
    "        \n",
    "        # init hyperparameters\n",
    "        hparams = argparse.Namespace()\n",
    "        \n",
    "        # lattice size\n",
    "        hparams.NT = NT\n",
    "        hparams.NX = NX\n",
    "\n",
    "        # dataloaders\n",
    "        hparams.num_workers = 0\n",
    "        \n",
    "        # name of the model\n",
    "        hparams.name = 'reg_' + name_string_helper + '_{}_training_samples'.format(len(train_data_subset))\n",
    "        \n",
    "        \"\"\"\n",
    "            Optimization hyperparameters\n",
    "        \"\"\"\n",
    "        # optimizer\n",
    "        hparams.lr = 1e-2\n",
    "        hparams.weight_decay = 0.\n",
    "        \n",
    "        # The total number of validation samples has to be divisible by the batch size\n",
    "        # for the loss function and the MSE losses to be correctly averaged at validation_epoch_end.\n",
    "        if len(train_data_subset) < 500:\n",
    "            hparams.batch_size = 50\n",
    "        else:\n",
    "            hparams.batch_size = 100\n",
    "        \n",
    "        hparams.test_batch_size = TEST_BATCH_SIZE\n",
    "        \n",
    "        \"\"\"\n",
    "            Architecture hyperparameters\n",
    "        \"\"\"\n",
    "        hparams.kernels = kernels\n",
    "        hparams.channels = channels\n",
    "        hparams.dense_sizes = dense_sizes\n",
    "\n",
    "        test_MSEs_per_trial = []\n",
    "        test_losses_per_trial = []\n",
    "        results_per_trial = []\n",
    "        \n",
    "        # The following boolean variable is used to print the number of paramters only once -\n",
    "        # right after the model is created.\n",
    "        printed_parameters = False\n",
    "\n",
    "        for trial in tqdm(range(NUM_TRIALS)):\n",
    "            # init model\n",
    "            model = ObsPredictor(hparams, train_data_subset, val_data_subset, test_data)\n",
    "\n",
    "            if not printed_parameters:\n",
    "                print(\"Number of trainable parameters: {}\".format(model.count_parameters()))\n",
    "                printed_parameters = True\n",
    "                \n",
    "            # tensorboard loggers\n",
    "            log_path = os.path.join(os.getcwd(), 'training_and_testing_logs')\n",
    "            if not os.path.isdir(log_path):\n",
    "                os.mkdir(log_path)\n",
    "                \n",
    "            log_name = hparams.name + \"_{:03d}\".format(trial)\n",
    "            tb = pl.loggers.TensorBoardLogger(save_dir=os.path.join(log_path, 'logs_' + name_string_helper), name=log_name)\n",
    "\n",
    "            # training\n",
    "            early_stopping = pl.callbacks.EarlyStopping(monitor='val_loss', min_delta=0., patience=25, mode='min')\n",
    "            checkpoint = pl.callbacks.model_checkpoint.ModelCheckpoint()\n",
    "            \n",
    "            trainer = pl.Trainer(gpus=1, min_epochs=MIN_EPOCHS, max_epochs=MAX_EPOCHS, check_val_every_n_epoch=1, benchmark=True,\n",
    "                                 weights_summary=None, progress_bar_refresh_rate=0, logger=tb,\n",
    "                                 early_stop_callback=early_stopping, checkpoint_callback=checkpoint)\n",
    "\n",
    "            trainer.fit(model)\n",
    "            \n",
    "            # testing\n",
    "            best_model = torch.load(checkpoint.best_model_path)\n",
    "            model.load_state_dict(best_model['state_dict'])\n",
    "            \n",
    "            model.eval()\n",
    "            trainer.test(model)\n",
    "            \n",
    "            test_MSE = model.vMSE\n",
    "            test_loss = model.vloss\n",
    "            test_MSEs_per_trial.append(test_MSE.numpy())\n",
    "            test_losses_per_trial.append(test_loss)\n",
    "            \n",
    "            # saving weights and hyperparameters\n",
    "            w = model.state_dict().copy()\n",
    "            result = {'weights': w, 'hparams': model.hparams.copy()}\n",
    "            results_per_trial.append(result)\n",
    "            \n",
    "        test_MSEs.append(test_MSEs_per_trial)\n",
    "        test_losses.append(test_losses_per_trial)\n",
    "        results.append(results_per_trial)\n",
    "        \n",
    "        pbar.update(1)\n",
    "        \n",
    "# save results\n",
    "pickle_path = os.path.join(os.getcwd(), 'test_pickles')\n",
    "if not os.path.isdir(pickle_path):\n",
    "    os.mkdir(pickle_path)\n",
    "    \n",
    "filename = name_string_helper + '.pickle'\n",
    "# If the file already exists, we do not want to overwrite it, but create a new one with a unique name.\n",
    "# To do this, we choose to prepend the current time.\n",
    "if os.path.isfile(os.path.join(pickle_path, filename)):\n",
    "    filename = str(time.time()) + filename\n",
    "    print('File already existed, timestamp was prepended to filename.')\n",
    "\n",
    "with open(os.path.join(pickle_path, filename), 'wb') as file:\n",
    "    pickle.dump([test_losses, test_MSEs, results, train_sample_numbers], file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on the other lattice sizes\n",
    "\n",
    "Now, the testing will be performed also on lattice sizes different from the one the training took place. The test results are saved to a .pickle file. The last entry of `train_sample_numbers` specifies the size of the training set. <br>\n",
    "Only `dims` should be changed, namely by choosing a subset of the available lattice sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions of the lattice the model shall be tested on\n",
    "dims = [(50,2), (60,4), (100,5)]#, (125,8), (200,10)]\n",
    "\n",
    "for dim in dims:\n",
    "    test_path = os.path.join(path_data, \"dataset-test-{:d}-{:d}.pt\".format(*dim))\n",
    "    if not os.path.isfile(test_path):\n",
    "        raise FileNotFoundError(f'There is no test set of this lattice size {(dim)} under the specified path.')\n",
    "        \n",
    "del test_MSEs\n",
    "del test_losses\n",
    "\n",
    "test_MSEs = []\n",
    "test_losses = []\n",
    "\n",
    "hparams = argparse.Namespace(**results[-1][0]['hparams'])\n",
    "\n",
    "with tqdm(total=len(dims)) as pbar:\n",
    "    for dim in dims:\n",
    "        print('Testing will be performed on the test set of the {}x{} lattice.'.format(*dim))\n",
    "\n",
    "        test_path = os.path.join(path_data, \"dataset-test-{:d}-{:d}.pt\".format(*dim))\n",
    "        test_data = torch.load(test_path)\n",
    "        print(\"Total test examples: {}\\n\".format(len(test_data)))\n",
    "        \n",
    "        test_MSEs_per_trial = []\n",
    "        test_losses_per_trial = []\n",
    "        \n",
    "        printed_parameters = False\n",
    "\n",
    "        for trial in tqdm(range(len(results[-1]))):\n",
    "            w = results[-1][trial]['weights']\n",
    "\n",
    "            model = ObsPredictor(hparams, None, None, test_data)\n",
    "            model.load_state_dict(w)\n",
    "\n",
    "            if not printed_parameters:\n",
    "                print(\"Number of trained parameters: {}\".format(model.count_parameters()))\n",
    "                printed_parameters = True\n",
    "    \n",
    "            trainer = pl.Trainer(gpus=1, logger=False, weights_summary=None)\n",
    "\n",
    "            trainer.test(model)\n",
    "            \n",
    "            test_MSE = model.vMSE\n",
    "            test_loss = model.vloss\n",
    "\n",
    "            test_MSEs_per_trial.append(test_MSE.numpy())\n",
    "            test_losses_per_trial.append(test_loss)\n",
    "\n",
    "        test_MSEs.append(test_MSEs_per_trial)\n",
    "        test_losses.append(test_losses_per_trial)\n",
    "        \n",
    "        pbar.update(1)\n",
    "        \n",
    "# save results\n",
    "filename = 'ls_' + name_string_helper + '.pickle'\n",
    "# If the file already exists, we do not want to overwrite it, but create a new one with a unique name.\n",
    "# To do this, we choose to prepend the current time.\n",
    "if os.path.isfile(os.path.join(pickle_path, filename)):\n",
    "    filename = str(time.time()) + filename\n",
    "    print('File already existed, timestamp was prepended to filename.')\n",
    "    \n",
    "with open(os.path.join(pickle_path, filename), 'wb') as file:\n",
    "    pickle.dump([test_losses, test_MSEs, dims], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('mcising')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "1e883a0688a9f85c2fceb484775ae18716030fcd47a73a14ca5f8f3e00df4b3a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
